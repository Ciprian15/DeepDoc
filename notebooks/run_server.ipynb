{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8889/ (Press CTRL+C to quit)\n",
      " * Restarting with inotify reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import base64, json\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# declare constants\n",
    "HOST = '0.0.0.0'\n",
    "PORT = 8889\n",
    "\n",
    "# initialize flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predict():\n",
    "    results = {\"prediction\" :\"Empty\", \"probability\" :{}}\n",
    "\n",
    "    # get data\n",
    "    input_img = BytesIO(base64.urlsafe_b64decode(request.form['img']))\n",
    "\n",
    "    # model.predict method takes the raw data and output a vector of probabilities\n",
    "    res =  model.predict(input_img)\n",
    "\n",
    "    results[\"prediction\"] = str(CLASS_MAPPING[np.argmax(res)])\n",
    "    results[\"probability\"] = float(np.max(res))*100\n",
    "    # results[\"prediction\"] = 5\n",
    "    # results[\"probability\"] = 50.424\n",
    "\n",
    "    # output data\n",
    "    return json.dumps(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run web server\n",
    "    app.run(host=HOST,debug=True,  # automatic reloading enabled\n",
    "            port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join(os.getcwd(), '../src')\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_doc as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.11111111111111"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10000*4/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.4.tar.gz (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytesseract) (7.0.0)\n",
      "Building wheels for collected packages: pytesseract\n",
      "  Building wheel for pytesseract (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytesseract: filename=pytesseract-0.3.4-py2.py3-none-any.whl size=13431 sha256=8c82e9fc087cac9c7a4ecc2fb0991e0703d822115f9943756818e9101624eacf\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/df/da/38/ab64f024f69d2c013ae96b75003fd874460698362ccdc054e1\n",
      "Successfully built pytesseract\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_bert import BertLayerNorm\n",
    "\n",
    "\n",
    "LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n",
    "\n",
    "LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n",
    "\n",
    "\n",
    "class LayoutlmConfig(BertConfig):\n",
    "    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
    "    model_type = \"bert\"\n",
    "\n",
    "    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_2d_position_embeddings = max_2d_position_embeddings\n",
    "\n",
    "\n",
    "class LayoutlmEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LayoutlmEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.x_position_embeddings = nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)\n",
    "        self.y_position_embeddings = nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)\n",
    "        self.h_position_embeddings = nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)\n",
    "        self.w_position_embeddings = nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,input_ids,bbox,token_type_ids=None,position_ids=None,inputs_embeds=None,):\n",
    "        seq_length = input_ids.size(1)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n",
    "        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n",
    "        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n",
    "        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n",
    "        h_position_embeddings = self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])\n",
    "        w_position_embeddings = self.w_position_embeddings(bbox[:, :, 2] - bbox[:, :, 0])\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = (\n",
    "            words_embeddings\n",
    "            + position_embeddings\n",
    "            + left_position_embeddings\n",
    "            + upper_position_embeddings\n",
    "            + right_position_embeddings\n",
    "            + lower_position_embeddings\n",
    "            + h_position_embeddings\n",
    "            + w_position_embeddings\n",
    "            + token_type_embeddings\n",
    "        )\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class LayoutlmModel(BertModel):\n",
    "\n",
    "    config_class = LayoutlmConfig\n",
    "    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    base_model_prefix = \"bert\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LayoutlmModel, self).__init__(config)\n",
    "        self.embeddings = LayoutlmEmbeddings(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        bbox,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = (head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1))\n",
    "                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = (head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1))  # We can specify head_mask for each layer\n",
    "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype)  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids)\n",
    "        encoder_outputs = self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
